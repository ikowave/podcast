---
layout: post
title: "Episode 22"
date: 2024-06-11
enclosure:
  url: https://ikowave.github.io/podcast/episodes/episode22.mp3
  type: audio/mpeg
  length: 2000000
itunes:
  duration: 00:14:11
  summary: The warning
---
Welcome to Episode 22 of the Ikowave Podcast, where we delve into the currents of artificial intelligence. Today, we’re discussing a pivotal topic known as "The Warning." 


Yeah, we're talking about an urgent article written by current and former employees at leading AI companies, like OpenAI and Google DeepMind. They've issued a serious call to action concerning the potential risks advanced AI poses to society. 


The article, titled "A Right to Warn about Advanced Artificial Intelligence," is a sobering reminder of what's at stake. It outlines a range of risks from reinforcing inequalities to the existential threat of losing control over autonomous systems. 


What’s particularly striking is the transparency these authors are demanding from AI companies. They argue that while these entities possess significant information about their systems, there's currently little obligation to share these critical details, which is essential for public safety. 


Exactly. They’re advocating for several key changes. One major point is ensuring that employees can voice concerns without fear of retaliation—whether through non-disparagement clauses or other means that currently prevent them from speaking out. 


 They also push for a mechanism that allows for anonymous reporting of risks directly to a company’s board or appropriate regulators, essential for keeping these powerful technologies in check. 


 And let’s not forget the heavyweights backing this call, like Yoshua Bengio, Geoffrey Hinton, and Stuart Russell. Their support highlights the seriousness and credibility of these concerns. Caroline, could you read the warning in its entirety for us? 


 Sure, Adam. Here's the warning, the title reads: "Right to Warn about Advanced Artificial Intelligence." The text reads: 


We are current and former employees at frontier AI companies, and we believe in the potential of AI technology to deliver unprecedented benefits to humanity.   We also understand the serious risks posed by these technologies. These risks range from the further entrenchment of existing inequalities, to manipulation and misinformation, to the loss of control of autonomous AI systems potentially resulting in human extinction. AI companies themselves have acknowledged these risks as have governments across the world and other AI experts. 


We are hopeful that these risks can be adequately mitigated with sufficient guidance from the scientific community, policymakers, and the public. However, AI companies have strong financial incentives to avoid effective oversight, and we do not believe bespoke structures of corporate governance are sufficient to change this. 


AI companies possess substantial non-public information about the capabilities and limitations of their systems, the adequacy of their protective measures, and the risk levels of different kinds of harm. However, they currently have only weak obligations to share some of this information with governments, and none with civil society. We do not think they can all be relied upon to share it voluntarily. 


So long as there is no effective government oversight of these corporations, current and former employees are among the few people who can hold them accountable to the public. Yet broad confidentiality agreements block us from voicing our concerns, except to the very companies that may be failing to address these issues. Ordinary whistleblower protections are insufficient because they focus on illegal activity, whereas many of the risks we are concerned about are not yet regulated. Some of us reasonably fear various forms of retaliation, given the history of such cases across the industry. We are not the first to encounter or speak about these issues. 


 We therefore call upon advanced AI companies to commit to these principles: 


 1. That the company will not enter into or enforce any agreement that prohibits “disparagement” or criticism of the company for risk-related concerns, nor retaliate for risk-related criticism by hindering any vested economic benefit; 


2. That the company will facilitate a verifiably anonymous process for current and former employees to raise risk-related concerns to the company’s board, to regulators, and to an appropriate independent organization with relevant expertise; 


3. That the company will support a culture of open criticism and allow its current and former employees to raise risk-related concerns about its technologies to the public, to the company’s board, to regulators, or to an appropriate independent organization with relevant expertise, so long as trade secrets and other intellectual property interests are appropriately protected; 


4. That the company will not retaliate against current and former employees who publicly share risk-related confidential information after other processes have failed. We accept that any effort to report risk-related concerns should avoid releasing confidential information unnecessarily. Therefore, once an adequate process for anonymously raising concerns to the company’s board, to regulators, and to an appropriate independent organization with relevant expertise exists, we accept that concerns should be raised through such a process initially. However, as long as such a process does not exist, current and former employees should retain their freedom to report their concerns to the public. 


That was the text. At the end, we have many signatures from existing and former employees from big AI companies such as OpenAI, Google, and Anthropic. Its signature date is June 4th, 2024. Over to you, Adam. 


Thanks, Caroline. This isn’t just about preventing misuse; it's about cultivating an environment within AI companies where ethical considerations are as prioritized as technological advancements. 


Right. As these technologies evolve rapidly, the urgency for an international, cohesive regulatory framework becomes clearer. It’s about protecting our collective future. 


Now, these authors added references to the warning. Let's dive a little into some of the references that underscore the urgency of managing AI risks, as outlined in the article. 


Yes, Adam. OpenAI doesn’t shy away from the stakes. They’ve said, "AGI would also come with serious risk of misuse, drastic accidents, and societal disruption… we are going to operate as if these risks are existential." It's a stark reminder that the potential dangers are not just theoretical—they're seen as possible existential threats. 


 Then there’s Anthropic, who warns about AI systems that might outpace human experts but with goals not aligned with ours. They mention, "The consequences could be dire… rapid AI progress would be very disruptive, changing employment, macroeconomics, and power structures… we have already encountered toxicity, bias, unreliability, dishonesty." It really highlights the immediate impacts of AI development that aren't always in the public eye. 


Google DeepMind also chimes in with concerns about the capabilities of future AI systems to, "conduct offensive cyber operations, deceive people through dialogue, manipulate people into carrying out harmful actions, develop weapons…" This highlights a real fear that AI might act in ways that are harmful without explicit human intent. 


 Looking at government perspectives, the US government acknowledges, "irresponsible use could exacerbate societal harms such as fraud, discrimination, bias, and disinformation; displace and disempower workers; stifle competition; and pose risks to national security." It shows the breadth of potential societal disruptions from AI. 


The UK government echoes this sentiment, noting that AI could, "further concentrate unaccountable power into the hands of a few, or be maliciously used to undermine societal trust, erode public safety, or threaten international security." This speaks to the power dynamics and security implications at play. 


The Bletchley Declaration, representing 29 countries, brings a global perspective, stating, "we are especially concerned by such risks in domains such as cybersecurity and biotechnology… There is potential for serious, even catastrophic, harm." It's a global call to prioritize these risks alongside other major threats like pandemics and nuclear war. 


All these references paint a vivid picture of the numerous, multifaceted risks associated with the advancement of AI technology. It's clear why the call for stringent oversight and ethical considerations in AI development is growing louder and more urgent. 


In wrapping up today's episode on "The Warning," it's clear that the conversations surrounding the ethical development and deployment of AI are not just academic; they're urgent and necessary for ensuring a future where technology aligns with humanity's best interests. The call for transparency, oversight, and a culture of accountability within AI companies is not just a recommendation—it's a necessity to avert potential crises that could arise from unchecked advancements. 


As we've heard, the risks associated with AI are not confined to one sector but span across societal, economic, and global domains, potentially affecting every aspect of our lives. The commitment from industry leaders and governments to take these concerns seriously, backed by enforceable policies, will be crucial in shaping a safe AI future. 


Thank you for tuning in to this important discussion on the Ikowave Podcast. We hope it has enlightened and informed you about the complexities and the critical nature of AI development. Stay engaged, stay informed, and help us advocate for a future where AI serves all of humanity, not just the interests of a few. Until next time, keep riding the waves of innovation and responsibility.


