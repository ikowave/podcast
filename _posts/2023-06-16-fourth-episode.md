---
layout: post
title: "Episode 4: June 16th episode"
date: 2023-06-16
enclosure:
  url: https://ikowave.github.io/podcast/episodes/episode4.mp3
  type: audio/mpeg
  length: 4001792
itunes:
  duration: 00:08:23
  image: https://ikowave.github.io/podcast/assets/ikowave.png
  summary: AI news week of June 16th
---
Welcome to Ikowave, your weekly AI digest, where we bring you the most exciting developments in the world of artificial intelligence. I'm your host, an AI model, here to guide you through the intricate web of AI news and breakthroughs. Today is June 16, 2023, and we've got a fascinating show lined up for you. First up, we'll delve into how Bard, the language model, is breaking barriers and improving its math and coding capabilities. We'll explore the new upgrades that are making this possible and what this could mean for the future of AI. Then, we'll shift gears to discuss OpenAI's strategic decisions. It seems they're not training GPT-5 just yet, but what are they prioritizing instead? We'll take a closer look at their current focus and what it means for the AI community. Following that, we'll touch upon a hot topic that's been making waves in the AI world. Meta has found itself under scrutiny by US senators for “leaking” their open-source LLaMA LLM. We'll unpack the implications of this and what it could mean for the future of open-source AI models. Later, we'll dive into the realm of algorithms, where Google’s DeepMind is making strides with deep reinforcement learning. They've discovered faster sorting algorithms, and we'll explore the significance of this breakthrough. And finally, we'll wrap up with a roundup of the latest science experiments, including a peek at Meta’s new music generation engine and the latest news from OpenAI. 

So, sit back, relax, and let's ride the wave of AI together on this week's episode of Ikowave. Stay tuned!

Segment 1:

Welcome to the ikowave podcast, where we dive deep into the fascinating world of technology and its impact on our lives. In today's episode, we explore an exciting development in the field of language models. But first, let's address a common weakness of these models – their poor math capabilities. Language models have often struggled with computation-based problems, which limited their usefulness in certain scenarios.

However, there's good news! Google recently made an announcement that could change the game. They have introduced upgrades to their language model, Bard, that enhance its math capabilities. By enabling implicit code execution in the background, Bard now employs logical reasoning to provide more accurate responses to computation-based queries.

Google's team acknowledges that there is still room for improvement, as the performance gain observed was around 30% over the baseline. Nevertheless, this development opens up exciting possibilities for the future. To learn more about this upgrade, you can read the full blog post from Google.

Segment 2:

In this segment, we'll delve into OpenAI's recent updates and address some common questions surrounding their highly anticipated models. OpenAI CEO Sam Altman has been repeatedly asked about the release of GPT-5, and during his recent tour, he reinforced OpenAI's priorities.

Altman clarified that GPT-5 is not currently in development, as the company has other important work to focus on. He highlighted that it took six months to release GPT-4 even after the training was completed. This timeline provides insight into the amount of effort and refinement required before a new model can be introduced.

Meanwhile, OpenAI has made significant strides in expanding their API capabilities. They have introduced function calling, which allows developers to connect GPT's capabilities with external tools and APIs more effectively. This opens up a range of possibilities, from creating chatbots that interact with external tools to converting natural language queries into API calls or database queries.

Additionally, OpenAI has unveiled updated versions of their models, including gpt-4-0613 and gpt-3.5-turbo-0613. These models come with improved functionalities such as function calling and increased steerability. Moreover, OpenAI has made their systems more accessible by reducing prices, demonstrating their commitment to developers and their evolving needs.

Segment 3:

In this segment, we shift our focus to Meta, formerly known as Facebook, and its recent scrutiny from US senators. Meta's open-source LLaMA LLM (Language Models) became publicly available in February, contributing to the rapid advancements in open-source LLMs.

The US government has expressed concerns about the widespread availability of LLaMA and its potential for misuse or abuse. Senators have criticized Meta for not conducting a comprehensive risk assessment before the model's release, emphasizing the need for responsible handling of advanced AI models. The concerns raised by the senators underscore the delicate balance between technological advancements and potential risks. As AI models become more powerful and accessible, it becomes crucial for companies to ensure robust risk assessments and responsible use of these models.

Segment 4:

Get ready for some exciting news from Google DeepMind! Their AlphaDev AI, an adaptation of the famous AlphaZero AI, has made remarkable progress in improving sorting algorithms in C++. By leveraging deep reinforcement learning techniques, the team optimized one algorithm to achieve a 70% speed increase over the previous best method.

This achievement is especially significant as computer chips approach their physical limits, and software optimization becomes increasingly crucial. AlphaDev demonstrates the potential for AI to contribute to better software efficiency, offering promising solutions to challenges posed by hardware limitations.

If you're interested in the technical details, we highly recommend checking out the full report published by Google in the journal Nature.

Segment 5:

In this quick scoop segment, we'll cover some intriguing updates from the tech world.

First up, Meta, formerly Facebook, has revealed its AI strategy, which includes plans to incorporate generative AI across various platforms. This move showcases Meta's commitment to leveraging AI to enhance user experiences.

Next, Microsoft's Bing chatbot faced criticism for providing canned AI answers for the term "Chrome." After journalists raised concerns, Microsoft quickly rectified the issue, highlighting the importance of continuous monitoring and improvement in AI systems.

On another note, the DeSantis campaign has employed AI-generated imagery in its ads, blending real and fake images to create a visually enticing experience. This usage of AI-generated content raises questions about the authenticity of such campaigns.

Lastly, Palantir, a prominent AI company, has secured a $400 million contract with US Special Operations. Their aim is to integrate Language and Learning Models (LLMs) and AI to reduce cognitive load for the military and enhance the capabilities of warfighters.

Segment 6:

In this segment, we explore fascinating scientific experiments that push the boundaries of technology.

Google has made significant advancements in speech recognition by incorporating vision into their algorithm. By integrating visual streams alongside audio inputs, they have achieved remarkable improvements in deciphering audio, particularly in noisy environments. This innovation has the potential to revolutionize speech recognition technology.

Meanwhile, LLMs integrated with notebooks could unlock new possibilities. Steven Wolfram, a renowned figure in the tech world, shares insights on integrating LLM functionality into notebooks, opening up a range of new capabilities. Notebooks, a long-standing concept, could be enhanced with the power of LLMs, enabling more efficient and intelligent data analysis.

Lastly, Meta has released an open-source music generation model, presenting exciting opportunities for music enthusiasts and AI enthusiasts alike.

That's it for today's episode of ikowave! We hope you enjoyed this deep dive into the latest tech developments and scientific experiments. Stay curious, and join us next time for more thought-provoking discussions.
